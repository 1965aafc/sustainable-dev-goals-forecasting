{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "import xgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import preprocess, missing, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206 indicators included\n"
     ]
    }
   ],
   "source": [
    "target = 'SI.POV.DDAY'\n",
    "predict_year=2010\n",
    "#percent of input Indicators to use (set to 100 for full set of input features)\n",
    "percent = 25\n",
    "\n",
    "#Load the data from disk\n",
    "input_dir = '.\\\\..\\\\data\\\\'\n",
    "data_input = \"cleaned_data.pkl\"\n",
    "data = pd.read_pickle(input_dir + data_input)\n",
    "\n",
    "#Possible subset of data choosen to reduce calulation time\n",
    "#For percetages less than 100% we try to choose a subset that represents the spread of variables\n",
    "\n",
    "if percent == 100:\n",
    "    pass\n",
    "else: \n",
    "    num_indicators_original = data.shape[1]\n",
    "    step = int(100/percent)\n",
    "    data_new = data.iloc[:,::step].copy()\n",
    "    #Add the target column if not already included\n",
    "    if target not in data_new.columns:\n",
    "        data_new[target] = data[target]\n",
    "    data = data_new\n",
    "    \n",
    "print(data.shape[1], \"indicators included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%time data_regressors, data_targets = \\\n",
    "        preprocess.window_data(data, lag=3,num_windows=10, step=1, predict_year=2010, \\\n",
    "                         target=target, impute_type='interpolation')\n",
    "\n",
    "#Break up into training and testing data.\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "data_train_regressors = data_regressors.loc[idx[:,2:10],:]\n",
    "data_train_targets = data_targets.loc[idx[:,2:10],:]\n",
    "data_test_regressors = data_regressors.loc[idx[:,1],:]\n",
    "data_test_targets= data_targets.loc[idx[:,1],:]\n",
    "\n",
    "#For Training, only consider windows that don't have a missing target as they offer nothing to training\n",
    "#Therefore, remove those observations from both the training regressors and targets datasets.\n",
    "data_train_regressors_subset = data_train_regressors[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "data_train_targets_subset = data_train_targets[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "\n",
    "#For testing, also remove windows with no target variable as it is impossible to measure preformance.\n",
    "data_test_regressors_subset = data_test_regressors[~np.isnan(list(data_test_targets.values.flatten()))]\n",
    "data_test_targets_subset = data_test_targets[~np.isnan(list(data_test_targets.values.flatten()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train_regressors_subset.values\n",
    "y_train = data_train_targets_subset.values.ravel()\n",
    "X_test = data_test_regressors_subset.values\n",
    "y_test = data_test_targets_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-the-box using the Scikit-learn interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:15] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE of XGBoost out-of-the-box is: 28.249442133560215\n"
     ]
    }
   ],
   "source": [
    "XGB = xgb.XGBRegressor(random_state=42)\n",
    "XGB.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = XGB.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"MSE of XGBoost out-of-the-box is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning of the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5\n",
    "\n",
    "scorer = make_scorer(mean_squared_error ,greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Tune the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:54:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:54:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:54:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:54:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:54:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=42,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(random_state=42, \n",
    "                       max_depth=5, \n",
    "                       min_child_weight = 1, \n",
    "                       gamma = 0, \n",
    "                       subsample=0.8, \n",
    "                       colsample_bytree = 0.8, \n",
    "                       scale_pos_weight = 1)\n",
    "\n",
    "param = model.get_xgb_params()\n",
    "data_matrix = xgb.DMatrix(X_train, label=y_train)\n",
    "cvresult = xgb.cv(param, data_matrix, num_boost_round=model.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=50)\n",
    "#Set the optimised number of estimators\n",
    "model.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:56:03] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MSE of xgboost after tuning (step 1) is: 25.366613176210667\n"
     ]
    }
   ],
   "source": [
    "best_so_far_model = model\n",
    "best_so_far_model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = best_so_far_model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"MSE of xgboost after tuning (step 1) is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=100, max_depth=5,\n",
    "                           min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                           objective='reg:squarederror', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "gsearch1 = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of xgboost after tuning (step 2) is: 24.77861493310224\n"
     ]
    }
   ],
   "source": [
    "best_so_far_model = gsearch1.best_estimator_\n",
    "best_so_far_model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = best_so_far_model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"MSE of xgboost after tuning (step 2) is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Tune Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "model = gsearch1.best_estimator_\n",
    "\n",
    "gsearch3 = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of xgboost after tuning (step 3) is: 24.77861493310224\n"
     ]
    }
   ],
   "source": [
    "best_so_far_model = gsearch3.best_estimator_\n",
    "best_so_far_model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = best_so_far_model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"MSE of xgboost after tuning (step 3) is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Tune Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'reg_alpha': [1e-05, 0.01, 0.1, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1]\n",
    "}\n",
    "\n",
    "model = gsearch3.best_estimator_\n",
    "\n",
    "grid_step4 = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,iid=False, cv=5 ,return_train_score=True)\n",
    "grid_step4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_alpha': 0.1}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_step4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 9.40506611,  9.37564592,  9.82336283, 10.1239759 ]),\n",
       " 'std_fit_time': array([0.1205106 , 0.07282836, 0.37370569, 0.29931526]),\n",
       " 'mean_score_time': array([0.01761298, 0.01501026, 0.02121539, 0.01681185]),\n",
       " 'std_score_time': array([0.00417924, 0.00384914, 0.00462477, 0.00487812]),\n",
       " 'param_reg_alpha': masked_array(data=[1e-05, 0.01, 0.1, 1],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'reg_alpha': 1e-05},\n",
       "  {'reg_alpha': 0.01},\n",
       "  {'reg_alpha': 0.1},\n",
       "  {'reg_alpha': 1}],\n",
       " 'split0_test_score': array([-6.40738403, -6.38016922, -6.64283264, -6.46345512]),\n",
       " 'split1_test_score': array([-17.27572003, -17.49499098, -17.67286162, -16.86207948]),\n",
       " 'split2_test_score': array([-34.68918529, -32.71073454, -34.75891266, -31.9309044 ]),\n",
       " 'split3_test_score': array([-11.62166216, -10.32274597, -10.60072565,  -9.73275872]),\n",
       " 'split4_test_score': array([-19.1275803 , -18.37638463, -17.74234072, -16.68877948]),\n",
       " 'mean_test_score': array([-17.82430636, -17.05700507, -17.48353466, -16.33559544]),\n",
       " 'std_test_score': array([9.54193399, 9.01462046, 9.62741285, 8.77122789]),\n",
       " 'rank_test_score': array([4, 2, 3, 1]),\n",
       " 'split0_train_score': array([-0.02329435, -0.02294999, -0.02255238, -0.03223238]),\n",
       " 'split1_train_score': array([-0.02572927, -0.02387367, -0.02859938, -0.04408154]),\n",
       " 'split2_train_score': array([-0.04530922, -0.04815272, -0.0458788 , -0.06951426]),\n",
       " 'split3_train_score': array([-0.02113644, -0.02201402, -0.02463633, -0.03683011]),\n",
       " 'split4_train_score': array([-0.02067527, -0.02011022, -0.02201745, -0.03247368]),\n",
       " 'mean_train_score': array([-0.02722891, -0.02742013, -0.02873687, -0.0430264 ]),\n",
       " 'std_train_score': array([0.00921624, 0.01044089, 0.00887781, 0.0139211 ])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_step4.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of xgboost after tuning (step 4) is: 24.639867157317262\n"
     ]
    }
   ],
   "source": [
    "best_so_far_model = grid_step4.best_estimator_\n",
    "best_so_far_model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = best_so_far_model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"MSE of xgboost after tuning (step 4) is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model with no Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5\n",
    "\n",
    "scorer = make_scorer(mean_squared_error ,greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%time data_regressors, data_targets = \\\n",
    "        preprocess.window_data(data, lag=3,num_windows=10, step=1, predict_year=2010, \\\n",
    "                         target=target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break up into training and testing data.\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "data_train_regressors = data_regressors.loc[idx[:,2:10],:]\n",
    "data_train_targets = data_targets.loc[idx[:,2:10],:]\n",
    "data_test_regressors = data_regressors.loc[idx[:,1],:]\n",
    "data_test_targets= data_targets.loc[idx[:,1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Training, only consider windows that don't have a missing target as they offer nothing to training\n",
    "#Therefore, remove those observations from both the training regressors and targets datasets.\n",
    "data_train_regressors_subset = data_train_regressors[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "data_train_targets_subset = data_train_targets[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "\n",
    "#For testing, also remove windows with no target variable as it is impossible to measure preformance.\n",
    "data_test_regressors_subset = data_test_regressors[~np.isnan(list(data_test_targets.values.flatten()))]\n",
    "data_test_targets_subset = data_test_targets[~np.isnan(list(data_test_targets.values.flatten()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_miss = data_train_regressors_subset.values\n",
    "y_train_miss = data_train_targets_subset.values.ravel()\n",
    "X_test_miss = data_test_regressors_subset.values\n",
    "y_test_miss = data_test_targets_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-the-box xgboost on data without imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of XGBoost out-of-the-box is: 22.089705414468636\n"
     ]
    }
   ],
   "source": [
    "XGB = xgb.XGBRegressor(random_state=42, n_estimators=100,  objective='reg:squarederror',max_depth=2)\n",
    "XGB.fit( X_train_miss,y_train_miss)\n",
    "#Make predictions\n",
    "predictions = XGB.predict(X_test_miss) \n",
    "\n",
    "mse= mean_squared_error(y_test_miss, predictions)\n",
    "print(\"MSE of XGBoost out-of-the-box is:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9928193439050952"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB.score(X_train_miss, y_train_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=100, max_depth=5,\n",
    "                           min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                           objective='reg:squarederror', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "grid_step2_missing = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,iid=False, cv=cv_folds)\n",
    "grid_step2_missing.fit(X_train_miss,y_train_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of xgboost after tuning (step 2) is: 22.470759551217924\n"
     ]
    }
   ],
   "source": [
    "best_so_far_model = grid_step2_missing.best_estimator_\n",
    "best_so_far_model.fit( X_train_miss,y_train_miss)\n",
    "#Make predictions\n",
    "predictions = best_so_far_model.predict(X_test_miss) \n",
    "\n",
    "mse= mean_squared_error(y_test_miss, predictions)\n",
    "print(\"MSE of xgboost after tuning (step 2) is:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth='3', min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=None, subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'reg_alpha': [0, 1e-05, 0.01, 0.1, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1]\n",
    "}\n",
    "\n",
    "model =  xgb.XGBRegressor(random_state=42, n_estimators=100,  objective='reg:squarederror',max_depth='3')\n",
    "\n",
    "grid_step4 = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,iid=False, cv=5 ,return_train_score=True)\n",
    "grid_step4.fit(X_train_miss,y_train_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_alpha': 0.01}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_step4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([12.46490774, 13.33110442, 13.2894845 , 14.97988238, 13.31553102]),\n",
       " 'std_fit_time': array([0.72104281, 0.68773044, 0.66761424, 0.80985538, 3.23645039]),\n",
       " 'mean_score_time': array([0.01417837, 0.01425643, 0.01100802, 0.01141543, 0.01285686]),\n",
       " 'std_score_time': array([0.00217707, 0.00973255, 0.00109672, 0.00205277, 0.00235153]),\n",
       " 'param_reg_alpha': masked_array(data=[0, 1e-05, 0.01, 0.1, 1],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'reg_alpha': 0},\n",
       "  {'reg_alpha': 1e-05},\n",
       "  {'reg_alpha': 0.01},\n",
       "  {'reg_alpha': 0.1},\n",
       "  {'reg_alpha': 1}],\n",
       " 'split0_test_score': array([-13.63875802, -13.63875679, -13.91272041, -15.06636568,\n",
       "        -15.41895681]),\n",
       " 'split1_test_score': array([-24.80837647, -24.8083728 , -21.9811472 , -21.98904935,\n",
       "        -21.91256844]),\n",
       " 'split2_test_score': array([-50.2840616 , -50.28405045, -51.14627544, -50.97711774,\n",
       "        -48.56329881]),\n",
       " 'split3_test_score': array([-17.67512002, -17.6751163 , -17.9715545 , -18.81298364,\n",
       "        -19.47994291]),\n",
       " 'split4_test_score': array([-34.25340739, -34.25340714, -33.59839134, -34.89704888,\n",
       "        -35.17958287]),\n",
       " 'mean_test_score': array([-28.1319447 , -28.13194069, -27.72201778, -28.34851306,\n",
       "        -28.11086997]),\n",
       " 'std_test_score': array([13.10444457, 13.10444183, 13.43096003, 13.13561274, 12.18412944]),\n",
       " 'rank_test_score': array([4, 3, 1, 5, 2]),\n",
       " 'split0_train_score': array([-0.11872867, -0.11872899, -0.13543702, -0.11182934, -0.11303704]),\n",
       " 'split1_train_score': array([-0.10426463, -0.10426489, -0.11426607, -0.10092619, -0.1006258 ]),\n",
       " 'split2_train_score': array([-0.11668201, -0.11668231, -0.11668314, -0.11865502, -0.12455442]),\n",
       " 'split3_train_score': array([-0.08938821, -0.08938851, -0.08820075, -0.09561031, -0.07558414]),\n",
       " 'split4_train_score': array([-0.11304181, -0.1130421 , -0.11814992, -0.12883363, -0.12552222]),\n",
       " 'mean_train_score': array([-0.10842107, -0.10842136, -0.11454738, -0.1111709 , -0.10786472]),\n",
       " 'std_train_score': array([0.01072722, 0.01072723, 0.01515349, 0.01196247, 0.01850148])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_step4.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of xgboost after tuning (step 2) is: 22.470759551217924\n"
     ]
    }
   ],
   "source": [
    "best_so_far_model = grid_step2_missing.best_estimator_\n",
    "best_so_far_model.fit( X_train_miss,y_train_miss)\n",
    "#Make predictions\n",
    "predictions = best_so_far_model.predict(X_test_miss) \n",
    "\n",
    "mse= mean_squared_error(y_test_miss, predictions)\n",
    "print(\"MSE of xgboost after tuning (step 2) is:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
